# MultiLayerPerceptron
A library developed to train deep neural nets written from scratch

##References

(1) Kingma, Diederik P., and Jimmy Ba. “Adam: A Method for Stochastic Optimization.” ArXiv.org, 30 Jan. 2017, arxiv.org/abs/1412.6980.<br>
(2) Skalski, Piotr. “Let's Code a Neural Network in Plain NumPy.” Towards Data Science, Towards Data Science, 12 Oct. 2018, towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795.

```
Epoch   0   Cost   2.58357209223   Accuracy   0.102595797281
Epoch   50   Cost   1.75090436199   Accuracy   0.613720642769
Saving Model...
MLP trained in 2.3542688999999997 seconds
Test batch accuracy: 0.826815642458
Reloaded
Epoch   0   Cost   1.72772859501   Accuracy   0.620519159456
Epoch   50   Cost   0.69413787185   Accuracy   0.862793572311
Epoch   100   Cost   0.372743086693   Accuracy   0.914709517923
MLP trained in 2.549563400000004 seconds
Test batch improved accuracy: 0.91061452514

```


![alt text](https://github.com/D-Thatcher/MultiLayerPerceptron/blob/master/accuracy_final.png)
![alt text](https://github.com/D-Thatcher/MultiLayerPerceptron/blob/master/entropy_final.png)

